{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification with Twitter NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Overview\n",
    "\n",
    "News agencies, governments and corporations sometimes track social media during natural disasters to try to monitor unfolding events. But because no single person or group of people can read all available Twitter data, organizations may turn to natural language processing methods to try and understand what is happening as disasters unfold. \n",
    "\n",
    "While this approach is powerful, inferring events from NLP can be tricky. For instance, say a person [tweets](https://twitter.com/AnyOtherAnnaK/status/629195955506708480) that \"LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE.\" This tweet includes the word \"ablaze\", which may signal to a computer that there is an unfolding disaster. However, in this particular case, the person is speaking metaphorically. A simple computer system using keywords (e.g. ablaze) might be fooled into thinking the tweet is reporting an actual fire.\n",
    "\n",
    "With this data I am predicting if a given tweet actually refers to a natural disaster. This exercise is motivated by real-world disaster monitoring systems, and uses a supervised binary classification and natural language processing.\n",
    "\n",
    "__Note__: This dataset originally comes from [Kaggle](https://www.kaggle.com/c/nlp-getting-started/overview). But it has been modified for this problem set. Information about the data from this problem set that you find on Kaggle will almost certainly be wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the problems and models\n",
    "Experimenting with perceptron and logistic regression in `sklearn`. Using `SGDClassifier` ([sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)),  implements stochastic gradient descent (SGD) for a variety of loss functions, including both perceptron and logistic regression, allowing them to easily move between the two classifiers.\n",
    "\n",
    "The code below loads the datasets. There are two data collections: the \"training\" data, which contains the tweets that will be used for training the classifiers, and the \"testing\" data, which are tweets that you will use to measure the classifier accuracy. The test tweets are instances that the classifier has never seen before. They represent a good way to see how the classifier will behave on data it hasn't seen before. However, the labels of the test tweets are known and can measure the accuracy.\n",
    "\n",
    "I included \"bag of words\" features, which are commonly used when doing classification with text. Each feature is a word, and the value of a feature for a particular tweet is number of times the word appears in the tweet (with value $0$ if the word does not appear in the tweet).\n",
    "\n",
    "A note on the labels: **If `Y_train` or `Y_test` are 1 this means the tweet refers to a real disaster; if the values are 0, it means the tweet does not refer to a real disaster** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>7585</td>\n",
       "      <td>10839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calgary Police Flood Road Closures in Calgary....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>7596</td>\n",
       "      <td>10851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>7601</td>\n",
       "      <td>10859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>7605</td>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>7610</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1493 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     id keyword location  \\\n",
       "0              1      4     NaN      NaN   \n",
       "1              7     13     NaN      NaN   \n",
       "2             16     24     NaN      NaN   \n",
       "3             18     26     NaN      NaN   \n",
       "4             26     38     NaN      NaN   \n",
       "...          ...    ...     ...      ...   \n",
       "1488        7585  10839     NaN      NaN   \n",
       "1489        7596  10851     NaN      NaN   \n",
       "1490        7601  10859     NaN      NaN   \n",
       "1491        7605  10864     NaN      NaN   \n",
       "1492        7610  10871     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                Forest fire near La Ronge Sask. Canada       1  \n",
       "1     I'm on top of the hill and I can see a fire in...       1  \n",
       "2                                         I love fruits       0  \n",
       "3                                     My car is so fast       0  \n",
       "4                                 Was in NYC last week!       0  \n",
       "...                                                 ...     ...  \n",
       "1488  Calgary Police Flood Road Closures in Calgary....       1  \n",
       "1489  RT @LivingSafely: #NWS issues Severe #Thunders...       1  \n",
       "1490  #breaking #LA Refugio oil spill may have been ...       1  \n",
       "1491  on the flip side I'm at Walmart and there is a...       1  \n",
       "1492  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "\n",
       "[1493 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# reading into the csv file train\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "# reads into the train columns target and text. target had 6120 1s, text has records of accidents\n",
    "Y_train = df_train[\"target\"]\n",
    "text_train = df_train[\"text\"]\n",
    "\n",
    "# X_train is a vector matrix with almost 90k elemnts, in row format\n",
    "vec = CountVectorizer()\n",
    "X_train = vec.fit_transform(text_train)\n",
    "\n",
    "# feature names is an array with weird values\n",
    "feature_names = np.asarray(vec.get_feature_names())\n",
    "\n",
    "# df test reads the test csv, Y_test is the target column, \n",
    "# lots of 1s. text_test is the text column of accidents.\n",
    "df_test = pd.read_csv('test.csv')\n",
    "Y_test = df_test[\"target\"]\n",
    "text_test = df_test[\"text\"]\n",
    "\n",
    "X_test = vec.transform(text_test)\n",
    "# these are the different tests below\n",
    "#Y_train\n",
    "#text_train\n",
    "#X_train\n",
    "#df_train\n",
    "#feature_names\n",
    "df_test\n",
    "#Y_test\n",
    "#text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `df_train` and `df_test` are dataframes that store the training (and testing) datasets, which are contained in csv files where the first column is the label and the second column is the text of the tweet.\n",
    "\n",
    "The [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class converts the raw text into a bag-of-words into a feature vector representation that `sklearn` can use.\n",
    "\n",
    "\n",
    "#### Deliverable 1.1\n",
    "\n",
    "How many training instances are in the dataset? How many test instances? \n",
    "\n",
    "    6120 training instances and 1493 testing instances\n",
    "\n",
    "#### Deliverable 1.2\n",
    "\n",
    "How many features are in the training data?\n",
    "\n",
    "    6 features are in the training data.\n",
    "\n",
    "#### Deliverable 1.3\n",
    "\n",
    "What is the distribution of labels in the training data? That is, what percentage of instances are about actual disasters?\n",
    "    \n",
    "        The percentage of instances that are natural disasters, is 43.2%. This is based on the data showing 2644 natural disasters and then dividing by the total instances, 6120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3476\n",
       "1    2644\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4320261437908497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2644 / 6120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDC Classifier\n",
    "The code below trains an [`SGDClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) using the perceptron loss, then it measures the accuracy of the classifier on the test data, using `sklearn`'s [`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function. \n",
    "\n",
    "The `fit` function trains the classifier. The feature weights are stored in the `coef_` variable after training. The `predict` function of the trained `SGDClassifier` outputs the predicted label for a given instance or list of instances.\n",
    "\n",
    "Additionally, this code displays the features and their weights in sorted order, which you may want to examine to understand what the classifier is learning. In general, in binary classification, the 0 class is considered the \"negative\" class.\n",
    "\n",
    "There are 3 keyword arguments that have been added to the code below. It is important you keep the same values of these arguments whenever you create an `SGDClassifier` instance in this assignment so that you get consistent results. They are:\n",
    "\n",
    "- `max_iter` is one of the stopping criteria, which is the maximum number of iterations/epochs the algorithm will run for.\n",
    "\n",
    "- `tol` is the other stopping criterion, which is how small the difference between the current loss and previous loss should be before stopping.\n",
    "\n",
    "- `random_state` is a seed for pseudorandom number generation. The algorithm uses randomness in the way the training data are sorted, which will affect the solution that is learned, and even the accuracy of that solution.\n",
    "\n",
    "Note: The loss function is convex, so the algorithm will find the same minimum regardless of how it is trained. Why is there random variation in the output? The reason is that even though there is only one minimum value of the loss, there may be different weights that result in the same loss, so randomness is a matter of tie-breaking. What's more, while different weights may have the same loss, they could lead to different classification accuracies, because the loss function is not the same as accuracy. (Unless accuracy was your loss function... which is possible, but uncommon because it turns out to be a difficult function to optimize.)\n",
    "Note that different computers may still give different answers, despite keeping these settings the same, because of how pseudorandom numbers are generated with different operating systems and Python environments.*\n",
    "\n",
    "To begin, run the code in the cell below without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SGD iterations: 35\n",
      "Training accuracy: 0.987908\n",
      "Testing accuracy: 0.781648\n",
      "\n",
      "Feature weights:\n",
      "\n",
      " - lowest\n",
      "\t zy3hpdjnwg: -0.7900\n",
      "\t qzlpfhpwdo: -0.6970\n",
      "\t better: -0.5112\n",
      "\t f7wqpcekg2: -0.5112\n",
      "\t sun: -0.5112\n",
      "\n",
      " - highest\n",
      "\t storm: 0.8829\n",
      "\t sunburned: 0.9294\n",
      "\t hurricane: 0.9294\n",
      "\t massacre: 1.0688\n",
      "\t earthquake: 1.2547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "classifier = SGDClassifier(loss='perceptron', max_iter=1000, tol=1.0e-12, random_state=123, eta0=100)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Number of SGD iterations: %d\" % classifier.n_iter_)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train, classifier.predict(X_train)))\n",
    "print(\"Testing accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "\n",
    "print(\"\\nFeature weights:\")\n",
    "args = np.argsort(classifier.coef_[0])\n",
    "\n",
    "print(\"\\n - lowest\")\n",
    "for a in args[0:5]:\n",
    "    print(\"\\t %s: %0.4f\" % (feature_names[a], classifier.coef_[0][a]))\n",
    "   \n",
    "print(\"\\n - highest\")\n",
    "for a in args[-5:]:\n",
    "    print(\"\\t %s: %0.4f\" % (feature_names[a], classifier.coef_[0][a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 2.1\n",
    "\n",
    "Based on the training accuracy, do you conclude that the data are (mostly) linearly separable? Why or why not?\n",
    "\n",
    "    Yes, because the number is statistically significant. The training accuracy is at 98.79%, which in a perceptron means it is linearly separable.\n",
    "\n",
    "#### Deliverable 2.2\n",
    "\n",
    "Which feature most increases the likelihood that the tweet does not refer to a real disaster, and which feature most increases the likelihood that the tweet refers to a real disaster? \n",
    "\n",
    "    The feature that most likely refers to a tweet of a real disatster is earthquake. It has the highest weight. While, zy3hpdjnwg has the lowest weight making it the least likely tweet to refer to a real disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 2.3 \n",
    "One technique for improving the resulting model with perceptron is to take an average of the weight vectors learned at different iterations of the algorithm, rather than only using the final weights that minimize the loss. That is, calculate $\\bar{\\mathbf{w}} = \\sum_{t=1}^T \\mathbf{w}^{(t)}$ where $\\mathbf{w}^{(t)}$ is the weight vector at iteration $t$ of the algorithm and $T$ is the number of iterations, and then use $\\bar{\\mathbf{w}}$ when making classifications on new data.\n",
    "\n",
    "To use this technique in your classifier, add the keyword argument `average=True` to the `SGDClassifier` function. Try it now using the cells below.\n",
    "\n",
    "Compare the initial training/test accuracies to the training/test accuracies after doing averaging. What happens? Why do you think averaging the weights from different iterations has this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9299999999999926"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 97.71 - 98.79\n",
    "b = 81.17 - 78.16 \n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SGD iterations: 35\n",
      "Training accuracy: 0.977124\n",
      "Testing accuracy: 0.811788\n",
      "\n",
      "Feature weights:\n",
      "\n",
      " - lowest\n",
      "\t full: -1.6394\n",
      "\t sun: -1.3797\n",
      "\t better: -1.3130\n",
      "\t also: -1.3088\n",
      "\t book: -1.2684\n",
      "\n",
      " - highest\n",
      "\t storm: 2.0315\n",
      "\t earthquake: 2.0326\n",
      "\t floods: 2.0351\n",
      "\t fires: 2.1129\n",
      "\t hiroshima: 2.1832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "classifier = SGDClassifier(loss='perceptron', max_iter=1000, tol=1.0e-12, random_state=123, eta0=100, average = True)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Number of SGD iterations: %d\" % classifier.n_iter_)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train, classifier.predict(X_train)))\n",
    "print(\"Testing accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "\n",
    "print(\"\\nFeature weights:\")\n",
    "args = np.argsort(classifier.coef_[0])\n",
    "\n",
    "print(\"\\n - lowest\")\n",
    "for a in args[0:5]:\n",
    "    print(\"\\t %s: %0.4f\" % (feature_names[a], classifier.coef_[0][a]))\n",
    "   \n",
    "print(\"\\n - highest\")\n",
    "for a in args[-5:]:\n",
    "    print(\"\\t %s: %0.4f\" % (feature_names[a], classifier.coef_[0][a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The training accuracy decreased to 97.71%, while the testing accuracy increased to 81.17%, for an overall increase in accuracy of approximately 2%. This is because the perceptron is combining the predicted outcomes of each iteration, and averages those outcomes for higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "For this problem, create a new `SGDClassifier`, this time setting the `loss` argument to `'log'`, which will train a logistic regression classifier. Set `average=False` for the remaining problems.\n",
    "\n",
    "Once you have trained the classifier, you can use the `predict` function to get the classifications, as with perceptron. Additionally, logistic regression provides probabilities for the predictions. You can get the probabilities by calling the `predict_proba` function. This will give a list of two numbers; the first is the probability that the class is $0$ and the second is the probability that the class is $1$.\n",
    "\n",
    "\n",
    "For the first task, add the keyword argument `alpha` to the `SGDClassifier` function. This is the regularization strength, called $\\lambda$ in lecture. If you don't specify `alpha`, it defaults to $0.0001$. Experiment with other values and see how this affects the outcome.\n",
    "\n",
    "#### Deliverable 3.1: \n",
    "\n",
    "Calculated the training and testing accuracy when `alpha` is one of $[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]$. Created a plot where the x-axis is `alpha` and the y-axis is accuracy, with two lines (one for training and one for testing). You can borrow the code from HW1 for generating plots in Python. Use [a log scale for the x-axis](https://matplotlib.org/examples/pylab_examples/log_demo.html) so that the `alpha` values are spaced evenly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# starter code\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "classifier = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 0.01, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "classifier.fit(X_train, Y_train)\n",
    " \n",
    "training_Accuracy = [0,0,0,0,0,0,0]\n",
    "\n",
    "testing_Accuracy = [0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "c1 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 0.0001, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "c1.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "c2 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 0.001, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "c2.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "c3 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 0.01, tol = 1.0e-12, random_state = 123, eta0 = 100, average=False)\n",
    "c3.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "c4 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 0.1, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "c4.fit(X_train, Y_train)\n",
    "\n",
    "c5 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 1.0, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "c5.fit(X_train, Y_train)\n",
    "\n",
    " \n",
    "c6 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 10, tol = 1.0e-12, random_state = 123, eta0 = 100, average = False)\n",
    "c6.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "c7 = SGDClassifier(loss = 'log', max_iter = 1000, alpha = 100, tol = 1.0e-12, random_state = 123, eta0 = 100, average=False)\n",
    "c7.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "training_Accuracy[0] = accuracy_score(Y_train, c1.predict(X_train))\n",
    "training_Accuracy[1] = accuracy_score(Y_train, c2.predict(X_train))\n",
    "training_Accuracy[2] = accuracy_score(Y_train, c3.predict(X_train))\n",
    "training_Accuracy[3] = accuracy_score(Y_train, c4.predict(X_train))\n",
    "training_Accuracy[4] = accuracy_score(Y_train, c5.predict(X_train))\n",
    "training_Accuracy[5] = accuracy_score(Y_train, c6.predict(X_train))\n",
    "training_Accuracy[6] = accuracy_score(Y_train, c7.predict(X_train))\n",
    "\n",
    "testing_Accuracy[0] = accuracy_score(Y_test, c1.predict(X_test))\n",
    "testing_Accuracy[1] = accuracy_score(Y_test, c2.predict(X_test))\n",
    "testing_Accuracy[2] = accuracy_score(Y_test, c3.predict(X_test))\n",
    "testing_Accuracy[3] = accuracy_score(Y_test, c4.predict(X_test))\n",
    "testing_Accuracy[4] = accuracy_score(Y_test, c5.predict(X_test))\n",
    "testing_Accuracy[5] = accuracy_score(Y_test, c6.predict(X_test))\n",
    "testing_Accuracy[6] = accuracy_score(Y_test, c7.predict(X_test))\n",
    "\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.semilogx(alpha, training_Accuracy)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.title('accuracy based on alpha')\n",
    "plt.plot(alpha, training_Accuracy, label = 'Training Accuracy', marker = 'o')\n",
    "plt.plot(alpha, testing_Accuracy, label = 'Tesing Accuracy', marker = 'o', color = \"blue\")\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 3.2\n",
    "\n",
    "Examine the classifier probabilities using the `predict_proba` function when training with different values of `alpha`. What do you observe? How does `alpha` affect the prediction probabilities, and why do you think this happens?\n",
    "\n",
    "    As the value of alpha increases the overall accuracy of predict_proba, and both the training and testing data gets worse. When alpha is at 0.0001 it is at its most accurate. Any smaller or larger and it becomes less accurate. This is because it calculates the penalty and 0.0001 fits the weights the most for optimizing the training and testing accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1073517 , 0.8926483 ],\n",
       "       [0.07474336, 0.92525664],\n",
       "       [0.04847949, 0.95152051],\n",
       "       ...,\n",
       "       [0.00976061, 0.99023939],\n",
       "       [0.04878059, 0.95121941],\n",
       "       [0.00382533, 0.99617467]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35321656, 0.64678344],\n",
       "       [0.16869801, 0.83130199],\n",
       "       [0.13702526, 0.86297474],\n",
       "       ...,\n",
       "       [0.03844073, 0.96155927],\n",
       "       [0.18279678, 0.81720322],\n",
       "       [0.03216899, 0.96783101]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53629373, 0.46370627],\n",
       "       [0.32635411, 0.67364589],\n",
       "       [0.30038956, 0.69961044],\n",
       "       ...,\n",
       "       [0.18228622, 0.81771378],\n",
       "       [0.32799007, 0.67200993],\n",
       "       [0.18481425, 0.81518575]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54681201, 0.45318799],\n",
       "       [0.46917619, 0.53082381],\n",
       "       [0.44543801, 0.55456199],\n",
       "       ...,\n",
       "       [0.44182658, 0.55817342],\n",
       "       [0.46094343, 0.53905657],\n",
       "       [0.41797239, 0.58202761]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51459256, 0.48540744],\n",
       "       [0.50241214, 0.49758786],\n",
       "       [0.49491548, 0.50508452],\n",
       "       ...,\n",
       "       [0.5094737 , 0.4905263 ],\n",
       "       [0.49789611, 0.50210389],\n",
       "       [0.49055035, 0.50944965]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c5.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50239332, 0.49760668],\n",
       "       [0.50106973, 0.49893027],\n",
       "       [0.50015043, 0.49984957],\n",
       "       ...,\n",
       "       [0.50220857, 0.49779143],\n",
       "       [0.50048597, 0.49951403],\n",
       "       [0.49968586, 0.50031414]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c6.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50056581, 0.49943419],\n",
       "       [0.50043237, 0.49956763],\n",
       "       [0.50033777, 0.49966223],\n",
       "       ...,\n",
       "       [0.50055279, 0.49944721],\n",
       "       [0.50037189, 0.49962811],\n",
       "       [0.50029223, 0.49970777]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c7.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9833333333333333,\n",
       " 0.9016339869281046,\n",
       " 0.8086601307189543,\n",
       " 0.7107843137254902,\n",
       " 0.6741830065359478,\n",
       " 0.6570261437908497,\n",
       " 0.5674836601307189]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8077695914266577,\n",
       " 0.8137977227059612,\n",
       " 0.7736101808439384,\n",
       " 0.695244474212994,\n",
       " 0.6651038178164769,\n",
       " 0.6617548559946417,\n",
       " 0.580040187541862]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 3.3: \n",
    "\n",
    "Now remove the `alpha` argument so that it goes back to the default value. We'll now look at the effect of the learning rate. By default, `sklearn` uses an \"optimal\" learning rate based on some heuristics that work well for many problems. However, it can be good to see how the learning rate can affect the algorithm.\n",
    "\n",
    "For this task, add the keyword argument `learning_rate` to the `SGDClassifier` function and set the value to `invscaling`. This defines the learning rate at iteration $t$ as: $\\eta_t = \\frac{\\eta_0}{t^a}$, where $\\eta_0$ and $a$ are both arguments you have to define in the `SGDClassifier` function, called `eta0` and `power_t`, respectively. Experiment with different values of `eta0` and `power_t` and see how they affect the number of iterations it takes the algorithm to converge. You will often find that it will not finish within the maximum of $1000$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnyparkerlee/opt/anaconda3/envs/ML/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=10000, learning_rate='invscaling', loss='log', power_t=2.0,\n",
       "              random_state=123, tol=1e-12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "classifier = SGDClassifier(loss = 'log', max_iter = 1000, eta0 = 10000, power_t = 2.0, learning_rate = \"invscaling\", tol = 1.0e-12, random_state = 123, average = False)\n",
    "classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the table below with the number of iterations for values of `eta0` in $[10.0, 100.0, 1000.0, 10000.0]$ and values of `power_t` in $[0.5, 1.0, 2.0]$. You may find it easier to write python code that can output the markdown for the table, but if you do that place the output here. If it does not converge within the maximum number of iterations (set to $1000$ by `max_iter`), record $1000$ as the number of iterations. You will need to read the documentation for this class to learn how to recover the actual number of iterations before reaching the stopping criterion.\n",
    "\n",
    "| `eta0`   | `power_t` | # Iterations |\n",
    "|:----------|:-:|:------------:|\n",
    "| $10.0$    | $0.5$     |      $177$         |\n",
    "| $10.0$    | $1.0$     |      $1000$       |\n",
    "| $10.0$    | $2.0$     |         $1000$      |\n",
    "| $100.0$   | $0.5$     |          $94$     |\n",
    "| $100.0$   | $1.0$     |          $1000$     |\n",
    "| $100.0$   | $2.0$     |           $1000$    |\n",
    "| $1000.0$  | $0.5$     |         $105$      |\n",
    "| $1000.0$  | $1.0$     |        $1000$      |\n",
    "| $1000.0$  | $2.0$     |         $1000$      |\n",
    "| $10000.0$ | $0.5$     |        $88$       |\n",
    "| $10000.0$ | $1.0$     |      $126$         |\n",
    "| $10000.0$ | $2.0$     |      $1000$         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 3.4\n",
    "\n",
    "Describe how `eta0` and `power_t` affect the learning rate based on the formula (e.g., if you increase `power_t`, what will this do to the learning rate?), and connect this to what you observe in the table above.\n",
    "\n",
    "    If you increase power_t the learning rate will decrease because it maxes out the iterations. In contrast, if you increase learning rate, power_t will decreases. This is because it takes more time to classify the algorithm and converge on the iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove the `learning_rate`, `eta0`, and `power_t` arguments so that the learning rate returns to the default setting. For this final task, we will experiment with how high the probabiity must be before an instance is classified as positive.\n",
    "\n",
    "The code below includes a function called `threshold` which takes as input the classification probabilities of the data (called `probs`, which is given by the function `predict_proba`) and a threshold (called `tau`, a scalar that should be a value between $0$ and $1$). It will classify each instance as $1$ if the probability of being $1$ is greater than `tau`, otherwise it will classify the instance as $0$. Note that if you set `tau` to $0.5$, the `threshold` function should give you exactly the same output as the classifier `predict` function.\n",
    "\n",
    "You should find that increasing the threshold causes the accuracy to drop. This makes sense, because you are classifying some things as 0 even though it's more probable that they are 1. So why do this? Suppose you care more about accurately identifying tweets about natural disasters than missing tweets about disasters (e.g. maybe you forward these tweets to first responders.) You thus want to be confident that when you classify a tweet as 1 that it really is 1.\n",
    "\n",
    "There is a metric called _precision_ which measures something like accuracy but for one specific class. Whereas accuracy is the percentage of tweets that were correctly classified, the precision of 1 would be the percentage of tweets classified as 1 that were correctly classified. (In other words, the number of tweets classified as 1 whose correct label was 1, divided by the number of tweets classified as 1.)\n",
    "\n",
    "Used the [`precision_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) function from `sklearn` to calculate the precision. It works similarly to the `accuracy_score` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 3.5\n",
    "\n",
    "Calculating the testing precision when the value of `tau` for thresholding is one of $[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]$. Created a plot where the x-axis is `tau` and the y-axis is precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 1 1]\n",
      "0.8090909090909091\n",
      "[1 0 0 ... 1 1 1]\n",
      "0.8481781376518218\n",
      "[1 0 0 ... 1 1 1]\n",
      "0.8906605922551253\n",
      "[1 0 0 ... 1 0 1]\n",
      "0.9205479452054794\n",
      "[1 0 0 ... 1 0 1]\n",
      "0.9547038327526133\n",
      "[1 0 0 ... 1 0 0]\n",
      "0.9723502304147466\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.9894736842105263\n"
     ]
    }
   ],
   "source": [
    "tau = [0.5 , 0.6, 0.7, 0.8, 0.9, 0.95, 0.99] \n",
    "for x in tau:\n",
    "    print(threshold(classifier.predict_proba(X_test), x))\n",
    "    print(precision_score(Y_test, threshold(classifier.predict_proba(X_test), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8090909090909091,\n",
       " 0.8481781376518218,\n",
       " 0.8906605922551253,\n",
       " 0.9205479452054794,\n",
       " 0.9547038327526133,\n",
       " 0.9723502304147466,\n",
       " 0.9894736842105263]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = [0,0,0,0,0,0,0]\n",
    "precision[0] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.5))\n",
    "precision[1] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.6))\n",
    "precision[2] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.7))\n",
    "precision[3] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.8))\n",
    "precision[4] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.9))\n",
    "precision[5] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.95))\n",
    "precision[6] = precision_score(Y_test, threshold(classifier.predict_proba(X_test), 0.99))\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use this function for deliverable 3.5\n",
    "# this function gives an array of 0 or 1. it is 1 if it is greater than tau and 0 if not.\n",
    "def threshold(probs, tau):\n",
    "    return np.where(probs[:,1] > tau, 1, 0)\n",
    "\n",
    "# your logistic regression code here\n",
    "\n",
    "classifier = SGDClassifier(loss = 'log', max_iter = 1000, tol = 1.0e-12, random_state = 123, average = False)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "plt.title('Threshold vs Precision')\n",
    "plt.plot(tau, precision, label = 'Precision Score', marker = 'o')\n",
    "plt.legend()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 3.6\n",
    "\n",
    "Describing the observations and what happens to the precision of the threshold as it increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As the precision score increases, the values increase for threshold, closer to 1. I believe this happens because accuracy_scores tells us if it was a natural disaster   or not, and the precision score tells us whether or not the score actually represented a natural disaster or not. In essence, Precision increased because the threshold was high, any tweets classified closer to 1 were in a higher threshold, and showed actual disasters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
